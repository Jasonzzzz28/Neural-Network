"""
I pledge on my honor that I have not given or received
any unauthorized assistance on this project.
Siyuan Zhang UID 115299634
Project 3 CMSC421
This file contains a simple Neural Network using back-propagation with reLU and sigmoid as activation functions for
hidden layer and output layer. The Neural Network also uses early stop, validation, randomization techniques for
optimization purposes.
"""
import math
import random
from copy import deepcopy

import numpy as np

global alpha, epoch, mse, resW, bia


def relu(x):
    '''
    ReLu function
    Input: a vector of elements to preform ReLU on
    Output: a vector of elements with ReLU preformed on them
    '''

    return np.maximum(x, 0)


def reluDer(x):
    '''
    derivative of ReLu function
    Input: a value
    Output: Derivative of ReLU on it
     '''
    if x >= 0:
        return 1
    else:
        return 0


def activation(z):
    '''
    Sigmoid function on a vector this is included for use as your activation function
    
    Input: a vector of elements to preform sigmoid on
    
    Output: a vector of elements with sigmoid preformed on them
    '''
    return 1 / (1 + np.exp(-z))


def sigderiv(z):
    '''
    The derivative of Sigmoid, you will need this to preform back prop
    '''
    return activation(z) * (1 - activation(z))


class NeuralNetwork(object):
    '''
    This Object outlines a basic neuralnetwork and the methods that it will need to function
    
    We have included an init method with a size parameter:
        Size: A 1D array indicating the node size of each layer
            E.G. Size = [2, 4, 1] Will instantiate weights and biases for a network
            with 2 input nodes, 1 hidden layer with 4 nodes, and an output layer with 1 node
        
        test_train defines the sizes of the input and output layers, but the rest is up to your implementation
    
    In this network for simplicity all nodes in a layer are connected to all nodes in the next layer, and the weights and
    biases and intialized as such. E.G. In a [2, 4, 1] network each of the 4 nodes in the inner layer will have 2 weight values
    and one biases value.
    
    '''

    def __init__(self, size, seed=42):
        '''
        Here the weights and biases specified above will be instantiated to random values
        Your network will change these values to fit a certain dataset by training
        '''
        self.seed = seed
        np.random.seed(self.seed)
        self.size = size
        self.weights = [np.random.randn(self.size[i], self.size[i - 1]) * np.sqrt(1 / self.size[i - 1]) for i in
                        range(1, len(self.size))]
        self.biases = [np.random.rand(n, 1) for n in self.size[1:]]

    def forward(self, input):
        '''
        Perform a feed forward computation 
        Parameters:

        input: data to be fed to the network with (shape described in spec)

        returns:

        the output value(s) of each example as ‘a’

        The values before activation was applied after the input was weighted as ‘pre_activations’

        The values after activation for all layers as ‘activations’

        You will need ‘pre_activaitons’ and ‘activations’ for updating the network
        '''
        a = input
        pre_activations = []
        activations = [a]
        for w, b in zip(self.weights, self.biases):
            z = np.dot(w, a) + b
            if len(activations) == len(self.weights):
                a = activation(z)
            else:
                a = relu(z)
            pre_activations.append(z)
            activations.append(a)
        global epoch
        epoch += 1
        return a, pre_activations, activations

    def backpropagate(self, X, y):
        '''
        Perform a backpropagation, calculate deltas and update weights using reLu and sigmoid
        input: X: data set; y: target
        '''
        global resW, bia
        for e in random.sample(range(0, int(len(X[0]) * 1)), int(len(X[0]) * 1)):
            deltas = [[0 for i in range(0, len(self.weights[l]))] for l in range(0, len(self.weights))]
            a, pre, post = self.forward([[X[i][e]] for i in range(0, len(X))])
            deltas[len(self.weights) - 1][0] = (y[0][e] - a[0][0]) * sigderiv(pre[len(pre) - 1][0][0])
            l = len(self.weights) - 2
            while l >= 0:
                for i in range(0, len(self.weights[l + 1])):
                    sum = 0
                    for j in range(0, len(self.weights[l + 1][i])):
                        sum += self.weights[l + 1][i][j] * deltas[l + 1][i]
                        deltas[l][j] = sum * reluDer(pre[l][i][0])
                l -= 1

            for l in range(0, len(self.weights)):
                for i in range(0, len(self.weights[l])):
                    for j in range(0, len(self.weights[l][i])):
                        self.weights[l][i][j] = self.weights[l][i][j] + alpha * (
                                post[l][j][0] * deltas[l][i])
                    self.biases[l][i][0] = self.biases[l][i][0] + 0.01* deltas[l][i]

            vtest = X[:, int(len(X[0]) * 0.9): len(X[0])]  # testing set
            pp = np.sum((self.predict(vtest) - y[0][int(len(X[0]) * 0.9): len(y[0])]) ** 2) / len(vtest[0])
            # calculate mean square error
            global mse
            if pp < mse:
                resW = deepcopy(self.weights)  # saving the best combination of weights
                bia = deepcopy(self.biases)
                mse = pp
            # if reach the max number of iterations
            if epoch > 150000 :
                print(mse)
                self.weights = resW  # set weight to the best weight combination
                self.biases = bia
                return False  # stop the loop
            # if mse < 0.03, then stop the loop
            if pp < 0.03:
                return False
        return True

    def train(self, X, y):
        '''
        Train the NN, stop when the cost is minimized
        Input: X: data set; y: target
        '''
        go = True
        while go:
            go = self.backpropagate(X, y)

    def predict(self, a):
        '''
       Input: a: list of list of input vectors to be tested
       
       This method will test a vector of input parameter vectors of the same form as X in test_train
       and return the results (Zero or One) that your trained network came up with for every element.
       
       This method does this the same way the included forward method moves an input through the network
       but without storying the previous values (which forward stores for use with the delta function you must write)
        '''
        for w, b in zip(self.weights, self.biases):
            z = np.dot(w, a) + b
            a = activation(z)
        predictions = (a > 0.5).astype(int)
        return predictions


'''
This is the function that we will call to test your network.

It must instantiate a network, which we include.

It must then train the network given the passed data, where x is the parameters in form:
        [[1rst parameter], [2nd parameter], [nth parameter]]
    
        Where if there are 100 training examples each of the n lists inside the list above will have 100 elements
        
    Y is the target which is guarenteed to be binary, or in other words true or false:
    Y will be of the form: 
        [[1, 0, 0, ...., 1, 0, 1]]
        
        (where 1 indicates true and zero indicates false)

'''


def test_train(X, y):
    inputSize = np.size(X, 0)

    # feel free to change the inside (hidden) layers to best suite your implementation
    # but the sizes of the input layer and output layer (inputSize and 1) must NOT CHANGE
    retNN = NeuralNetwork([inputSize, 12, 1])

    # initializing global values
    global alpha, epoch, mse, resW, bia
    alpha = 0.21#0.21 #0.71
    epoch = 0
    mse = math.inf
    resW = 0
    bia = 0
    # train your network here
    retNN.train(X, y)

    # then the function MUST return your TRAINED nueral network
    return retNN
